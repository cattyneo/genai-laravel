# GenAI Models Configuration
# This file contains all model definitions for different providers
# Last updated: 2025-06-13 16:06:46
# Updated by: genai:model-update command
openai:
  gpt-4:
    provider: openai
    model: gpt-4
    type: text
    features:
      - streaming
      - function_calling
      - system_message
    limits:
      max_tokens: 8192
      context_window: 128000
  gpt-3.5:
    provider: openai
    model: gpt-3.5-turbo
    type: text
    features:
      - streaming
      - function_calling
      - system_message
    limits:
      max_tokens: 4096
      context_window: 4096
  gpt-4o:
    provider: openai
    model: gpt-4o
    type: text
    features:
      - streaming
      - function_calling
      - system_message
      - vision
      - structured_output
    limits:
      max_tokens: 16384
      context_window: 1000000
  gpt-4.1:
    provider: openai
    model: gpt-4.1
    type: text
    features:
      - streaming
      - function_calling
      - system_message
      - vision
      - structured_output
    limits:
      max_tokens: 16384
      context_window: 1000000
  gpt-image-1:
    provider: openai
    model: gpt-image-1
    type: image
    features:
      - streaming
  codex:
    provider: openai
    model: codex-mini-latest
    type: text
    features:
      - streaming
  davinci:
    provider: openai
    model: davinci-002
    type: text
    features:
      - streaming
  babbage:
    provider: openai
    model: babbage-002
    type: text
    features:
      - streaming
  dall-e-3:
    provider: openai
    model: dall-e-3
    type: image
    features:
      - image_generation
  dall-e-2:
    provider: openai
    model: dall-e-2
    type: image
    features:
      - image_generation
  tts-1:
    provider: openai
    model: tts-1
    type: audio
    features:
      - text_to_speech
  text-embedding-3:
    provider: openai
    model: text-embedding-3-small
    type: embedding
    features:
      - streaming
  chatgpt-4o:
    provider: openai
    model: chatgpt-4o-latest
    type: text
    features:
      - streaming
      - function_calling
      - system_message
      - vision
      - structured_output
    limits:
      max_tokens: 16384
      context_window: 1000000
  o1:
    provider: openai
    model: o1
    type: text
    features:
      - streaming
      - reasoning
  omni:
    provider: openai
    model: omni-moderation-latest
    type: text
    features:
      - streaming
  gpt-4o-mini:
    provider: openai
    model: gpt-4o-mini-realtime-preview-2024-12-17
    type: text
    features:
      - streaming
      - function_calling
      - system_message
      - vision
      - structured_output
    limits:
      max_tokens: 16384
      context_window: 1000000
  computer-use:
    provider: openai
    model: computer-use-preview
    type: text
    features:
      - streaming
  o3:
    provider: openai
    model: o3
    type: text
    features:
      - streaming
      - reasoning
    limits:
      max_tokens: 100000
      context_window: 200000
  gpt-4.5:
    provider: openai
    model: gpt-4.5-preview
    type: text
    features:
      - streaming
      - function_calling
      - system_message
    limits:
      max_tokens: 8192
      context_window: 128000
  o4:
    provider: openai
    model: o4-mini-2025-04-16
    type: text
    features:
      - streaming
      - reasoning
    limits:
      max_tokens: 65536
      context_window: 128000
  gpt-3.5-turbo-16k:
    provider: openai
    model: gpt-3.5-turbo-16k
    type: text
    features:
      - streaming
      - function_calling
      - system_message
    limits:
      max_tokens: 4096
      context_window: 16384
  whisper-1:
    provider: openai
    model: whisper-1
    type: audio
    features:
      - transcription
  text-embedding-ada:
    provider: openai
    model: text-embedding-ada-002
    type: embedding
    features:
      - streaming
gemini:
  embedding:
    provider: gemini
    model: 'Embedding Gecko'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
    limits:
      max_tokens: 1
      context_window: 1024
  gemini-1.0-pro:
    provider: gemini
    model: 'Gemini 1.0 Pro Vision'
    type: vision
    features:
      - streaming
      - function_calling
      - structured_output
      - reasoning
      - content_generation
    limits:
      max_tokens: 4096
      context_window: 12288
  gemini-pro:
    provider: gemini
    model: 'Gemini 1.0 Pro Vision'
    type: vision
    features:
      - streaming
      - function_calling
      - structured_output
      - reasoning
      - content_generation
    limits:
      max_tokens: 4096
      context_window: 12288
  gemini-1.5-pro:
    provider: gemini
    model: 'Gemini 1.5 Pro Latest'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - reasoning
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 2000000
  gemini-1.5:
    provider: gemini
    model: 'Gemini 1.5 Pro'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - reasoning
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 2000000
  gemini-1.5-flash-001:
    provider: gemini
    model: 'Gemini 1.5 Flash 001 Tuning'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 16384
  gemini-1.5-flash-8b:
    provider: gemini
    model: 'Gemini 1.5 Flash-8B 001'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 1000000
  gemini-2.5-pro-exp:
    provider: gemini
    model: 'Gemini 2.5 Pro Experimental 03-25'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - reasoning
      - content_generation
    limits:
      max_tokens: 65536
      context_window: 1048576
  gemini-2.5-pro-preview:
    provider: gemini
    model: 'Gemini 2.5 Pro Preview 03-25'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - reasoning
      - content_generation
    limits:
      max_tokens: 65536
      context_window: 1048576
  gemini-2.5-flash-preview:
    provider: gemini
    model: 'Gemini 2.5 Flash Preview 04-17'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - content_generation
    limits:
      max_tokens: 65536
      context_window: 1048576
  gemini-2.0:
    provider: gemini
    model: 'Gemini 2.0 Flash Experimental'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 1048576
  gemini-2.0-flash-exp:
    provider: gemini
    model: 'Gemini 2.0 Flash (Image Generation) Experimental'
    type: vision
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 1048576
  gemini-2.0-flash:
    provider: gemini
    model: 'Gemini 2.0 Flash-Lite 001'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 1048576
  gemini-2.0-flash-lite-preview:
    provider: gemini
    model: 'Gemini 2.0 Flash-Lite Preview 02-05'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 1048576
  gemini-2.0-pro:
    provider: gemini
    model: 'Gemini 2.0 Pro Experimental'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - reasoning
      - content_generation
    limits:
      max_tokens: 65536
      context_window: 1048576
  gemini-2.0-pro-exp:
    provider: gemini
    model: 'Gemini 2.0 Pro Experimental 02-05'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - reasoning
      - content_generation
    limits:
      max_tokens: 65536
      context_window: 1048576
  gemini:
    provider: gemini
    model: 'Gemini Experimental 1206'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - content_generation
    limits:
      max_tokens: 65536
      context_window: 1048576
  gemini-2.0-flash-thinking-exp:
    provider: gemini
    model: 'Gemini 2.5 Flash Preview 04-17'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - content_generation
    limits:
      max_tokens: 65536
      context_window: 1048576
  gemini-2.5-flash:
    provider: gemini
    model: 'Gemini 2.5 Flash Preview TTS'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 32768
  gemini-2.5-pro:
    provider: gemini
    model: 'Gemini 2.5 Pro Preview TTS'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - reasoning
      - content_generation
    limits:
      max_tokens: 65536
      context_window: 65536
  learnlm-2.0:
    provider: gemini
    model: 'LearnLM 2.0 Flash Experimental'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
      - content_generation
    limits:
      max_tokens: 32768
      context_window: 1048576
  gemma-3-1b-it:
    provider: gemini
    model: 'Gemma 3 1B'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 32768
  gemma-3-4b-it:
    provider: gemini
    model: 'Gemma 3 4B'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 32768
  gemma-3-12b-it:
    provider: gemini
    model: 'Gemma 3 12B'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 32768
  gemma-3-27b-it:
    provider: gemini
    model: 'Gemma 3 27B'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - content_generation
    limits:
      max_tokens: 8192
      context_window: 131072
  gemma-3n-e4b-it:
    provider: gemini
    model: 'Gemma 3n E4B'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - content_generation
    limits:
      max_tokens: 2048
      context_window: 8192
  text:
    provider: gemini
    model: 'Text Embedding 004'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
    limits:
      max_tokens: 1
      context_window: 2048
  gemini-embedding-exp:
    provider: gemini
    model: 'Gemini Embedding Experimental 03-07'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
    limits:
      max_tokens: 1
      context_window: 8192
  aqa:
    provider: gemini
    model: 'Model that performs Attributed Question Answering.'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
    limits:
      max_tokens: 1024
      context_window: 7168
  imagen-3.0:
    provider: gemini
    model: 'Imagen 3.0 002 model'
    type: vision
    features:
      - streaming
      - function_calling
      - structured_output
    limits:
      max_tokens: 8192
      context_window: 480
  veo-2.0:
    provider: gemini
    model: 'Veo 2'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
    limits:
      max_tokens: 8192
      context_window: 480
  gemini-2.5-flash-preview-native-audio:
    provider: gemini
    model: 'Gemini 2.5 Flash Preview Native Audio Dialog'
    type: text
    features:
      - streaming
      - function_calling
      - structured_output
      - vision
      - grounding
    limits:
      max_tokens: 8192
      context_window: 131072
claude:
  claude-opus-4:
    provider: claude
    model: 'Claude Opus 4'
    type: text
    features:
      - streaming
      - vision
      - function_calling
      - structured_output
      - reasoning
      - advanced_reasoning
    limits:
      max_tokens: 32000
      context_window: 200000
  claude-sonnet-4:
    provider: claude
    model: 'Claude Sonnet 4'
    type: text
    features:
      - streaming
      - vision
      - function_calling
      - structured_output
      - reasoning
      - advanced_reasoning
    limits:
      max_tokens: 64000
      context_window: 200000
  claude-3-7:
    provider: claude
    model: 'Claude Sonnet 3.7'
    type: text
    features:
      - streaming
      - vision
      - function_calling
      - advanced_reasoning
    limits:
      max_tokens: 4096
      context_window: 200000
  claude-3-5:
    provider: claude
    model: 'Claude Sonnet 3.5 (New)'
    type: text
    features:
      - streaming
      - vision
      - function_calling
      - structured_output
      - advanced_reasoning
    limits:
      max_tokens: 8192
      context_window: 200000
  claude-3:
    provider: claude
    model: 'Claude Haiku 3'
    type: text
    features:
      - streaming
      - vision
      - function_calling
      - fast_response
    limits:
      max_tokens: 4096
      context_window: 200000
  claude-2.1:
    provider: claude
    model: 'Claude 2.1'
    type: text
    features:
      - streaming
    limits:
      max_tokens: 4096
      context_window: 200000
  claude-2.0:
    provider: claude
    model: 'Claude 2.0'
    type: text
    features:
      - streaming
    limits:
      max_tokens: 4096
      context_window: 200000
grok:
  grok-2:
    provider: grok
    model: grok-2-1212
    type: text
    features:
      - streaming
    limits:
      max_tokens: 131072
      context_window: 131072
  grok-3:
    provider: grok
    model: grok-3
    type: text
    features:
      - streaming
      - function_calling
      - reasoning
    limits:
      max_tokens: 131072
      context_window: 1000000
